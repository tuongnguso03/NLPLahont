{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wheel\n",
    "!pip install pandas\n",
    "!pip install nucleus-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nucleus_api\n",
    "\n",
    "configuration = nucleus_api.Configuration()\n",
    "# configuration.host = 'localhost:5000'\n",
    "configuration.host = 'http://localhost:5000/'\n",
    "configuration.api_key['x-api-key'] = 'VRgaGSWm'\n",
    "\n",
    "api_instance = nucleus_api.NucleusApi(nucleus_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, urllib.request\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import arxiv\n",
    "\n",
    "def search():\n",
    "    # Define the base URL for the arXiv API\n",
    "    base_url = \"http://export.arxiv.org/api/query?\"\n",
    "    # we can limit the number of results like this\n",
    "    nrPapers = 10\n",
    "    # Construct the full URL for the search\n",
    "    search_url = base_url + \"search_query=all:e&start=0&max_results=\" + str(nrPapers) + \"&sortBy=lastUpdatedDate&sortOrder=descending\"\n",
    "    # get the raw data from the query\n",
    "    data = urllib.request.urlopen(search_url)\n",
    "    rawData = str(data.read().decode('utf-8'))\n",
    "\n",
    "    # collect the links of newest papers\n",
    "    papers = []\n",
    "    entryMatches = re.finditer(r\"<entry>(.*?)</entry>\", rawData, flags=re.DOTALL)\n",
    "    for entry in entryMatches:\n",
    "        link = \"\"\n",
    "        linkMatches = re.finditer(r\"<id>(.*?)</id>\", str(entry.group(1)), flags=re.DOTALL)\n",
    "        for l in linkMatches:\n",
    "            link = l.group(1)\n",
    "        link = link.replace(\"\\n\", \"\")\n",
    "        #print(\"Search complete!\")\n",
    "        papers.append(link)\n",
    "    \n",
    "    return papers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = search()\n",
    "for paper_link in papers:\n",
    "    paper = next(arxiv.Search(id_list=[paper_link[21:]]).results())\n",
    "    paper.download_pdf(dirpath=\"./fetched_pdfs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "fname = \"./fetched_pdfs/2212.08015v1.First_detection_of_the_outer_edge_of_an_AGN_accretion_disc_Very_fast_multiband_optical_variability_of_NGC_4395_with_GTC_HiPERCAM_and_LT_IO_O.pdf\"  # get document filename\n",
    "doc = fitz.open(fname)  # open document\n",
    "out = open(fname + \".txt\", \"wb\")  # open text output\n",
    "for page in doc:  # iterate the document pages\n",
    "    text = page.get_text().encode(\"utf8\")  # get plain text (is in UTF-8)\n",
    "    out.write(text)  # write text of page\n",
    "    out.write(bytes((12,)))  # write page delimiter (form feed 0x0C)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz\n",
    "import json\n",
    "from nucleus_api.rest import ApiException\n",
    "dataset = \"demos\"\n",
    "for fname in os.listdir(\"./fetched_pdfs\"):\n",
    "    path = \"./fetched_pdfs/\" + fname\n",
    "    newdocument = {}\n",
    "    newdocument[\"title\"] = filename[13:-4].replace(\"_\",\" \")\n",
    "    newdocument[\"time\"] = \"12/1/2023\"\n",
    "    newdocument[\"content\"] = \"\"\n",
    "    doc = fitz.open(path)\n",
    "    for page in doc:  # iterate the document pages\n",
    "        newdocument[\"content\"] += str(page.getText().encode(\"utf8\"))\n",
    "    \n",
    "    payload = nucleus_api.Appendjsonparams(dataset=dataset,\n",
    "                                        document=newdocument)\n",
    "    try:\n",
    "        api_response = api_instance.post_append_json_to_dataset(payload)\n",
    "        print(api_response.result)\n",
    "    except ApiException as e:\n",
    "        api_error = json.loads(e.body)\n",
    "        print('ERROR:', api_error['message'])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"fetched_pdfs\"\n",
    "dataset = \"demos\"\n",
    "nucleus.Dataset.delete_dataset(dataset)\n",
    "for i in os.listdir(path):\n",
    "    if i.endswith(\".pdf\"):\n",
    "        api_response = api_instance.post_upload_file(path + \"/\" + i, dataset)\n",
    "        fp = api_response.result\n",
    "        print(fp.filename, '(', fp.size, 'bytes) has been added to dataset', dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute topics:\n",
    "payload = nucleus_api.Topics(dataset=dataset, num_topics=5)\n",
    "api_response = api_instance.post_topic_api(payload) \n",
    "#create a json file topics.json containing only topic keywords, weights and strength\n",
    "topics = [{'keywords':topic.keywords, \n",
    "           'keywords_weight':topic.keywords_weight, \n",
    "           'strength':topic.strength} for topic in api_response.result.topics]\n",
    "print(json.dumps(topics, indent=4, ensure_ascii=False))\n",
    "# with open('demo_topics.json', 'w') as outfile:\n",
    "with open('/demo/demo_topics.json', 'w') as outfile:\n",
    "    json.dump(topics, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
